{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues and Eigenvectors\n",
    "\n",
    "## Definition\n",
    "\n",
    "Given a square matrix \\( A \\) of size \\( n \\times n \\), an **eigenvector** \\( \\mathbf{v} \\) and the corresponding **eigenvalue** \\( \\lambda \\) satisfy the following equation:\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- \\( A \\) is the square matrix.\n",
    "- \\( \\mathbf{v} \\) is the eigenvector.\n",
    "- \\( \\lambda \\) is the eigenvalue.\n",
    "\n",
    "### Eigenvalue Equation\n",
    "\n",
    "The above equation can also be written as:\n",
    "\n",
    "$$\n",
    "(A - \\lambda I) \\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( I \\) is the identity matrix of the same size as \\( A \\).\n",
    "- \\( \\lambda \\) is the eigenvalue, and \\( \\mathbf{v} \\) is the corresponding eigenvector.\n",
    "\n",
    "For non-trivial solutions (\\( \\mathbf{v} \\neq 0 \\)), the determinant of the matrix \\( (A - \\lambda I) \\) must be zero. This leads to the **characteristic equation**:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "### Characteristic Polynomial\n",
    "\n",
    "The determinant expression \\( \\det(A - \\lambda I) = 0 \\) is a **polynomial** in \\( \\lambda \\), called the **characteristic polynomial** of matrix \\( A \\). The degree of this polynomial is equal to the size of the matrix, \\( n \\).\n",
    "\n",
    "- For a matrix \\( A \\) of size \\( n \\times n \\), the characteristic polynomial is:\n",
    "  \n",
    "  $$\n",
    "  p(\\lambda) = \\det(A - \\lambda I) = 0\n",
    "  $$\n",
    "\n",
    "This polynomial can be solved for the values of \\( \\lambda \\) (the eigenvalues). The solutions to this equation are the eigenvalues of \\( A \\).\n",
    "\n",
    "### Eigenvalue and Eigenvector Problem\n",
    "\n",
    "Once the eigenvalues \\( \\lambda \\) are found by solving the characteristic equation, the corresponding **eigenvectors** can be found by solving the system of linear equations:\n",
    "\n",
    "$$\n",
    "(A - \\lambda I) \\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "This is a homogeneous system of linear equations, where \\( \\mathbf{v} \\) is the eigenvector corresponding to the eigenvalue \\( \\lambda \\).\n",
    "\n",
    "### Eigenvalue Decomposition\n",
    "\n",
    "Eigenvalue decomposition expresses a matrix \\( A \\) in terms of its eigenvalues and eigenvectors. If \\( A \\) has \\( n \\) linearly independent eigenvectors, it can be decomposed as:\n",
    "\n",
    "$$\n",
    "A = V \\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( V \\) is the matrix whose columns are the eigenvectors of \\( A \\).\n",
    "- \\( \\Lambda \\) is a diagonal matrix with the eigenvalues of \\( A \\) on the diagonal.\n",
    "\n",
    "### Spectral Theorem\n",
    "\n",
    "For a symmetric matrix \\( A \\) (i.e., \\( A = A^T \\)), the **spectral theorem** states that \\( A \\) can be diagonalized as:\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\) is an orthogonal matrix (i.e., \\( Q^{-1} = Q^T \\)) whose columns are the normalized eigenvectors of \\( A \\).\n",
    "- \\( \\Lambda \\) is a diagonal matrix of the eigenvalues.\n",
    "\n",
    "### Diagonalization\n",
    "\n",
    "A matrix \\( A \\) is said to be **diagonalizable** if there exists an invertible matrix \\( P \\) such that:\n",
    "\n",
    "$$\n",
    "A = P D P^{-1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P \\) is the matrix of eigenvectors.\n",
    "- \\( D \\) is a diagonal matrix containing the eigenvalues.\n",
    "\n",
    "This is equivalent to the equation for eigenvalue decomposition:\n",
    "\n",
    "$$\n",
    "A = V \\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "### Power Method (for Largest Eigenvalue)\n",
    "\n",
    "One iterative method to find the largest eigenvalue is the **power method**. Given an initial vector \\( \\mathbf{v_0} \\), the power method proceeds as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v_{k+1}} = A \\mathbf{v_k}\n",
    "$$\n",
    "\n",
    "The algorithm converges to the eigenvector corresponding to the largest eigenvalue. The corresponding eigenvalue can be approximated by:\n",
    "\n",
    "$$\n",
    "\\lambda \\approx \\frac{\\mathbf{v_k}^T A \\mathbf{v_k}}{\\mathbf{v_k}^T \\mathbf{v_k}}\n",
    "$$\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "While not directly related to eigenvalues/eigenvectors of square matrices, **Singular Value Decomposition (SVD)** is a closely related concept often used in data science. It decomposes any \\( m \\times n \\) matrix \\( A \\) into three matrices:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( U \\) and \\( V \\) are orthogonal matrices (similar to eigenvector matrices).\n",
    "- \\( \\Sigma \\) is a diagonal matrix containing the singular values of \\( A \\), which are the square roots of the eigenvalues of \\( A^T A \\) and \\( A A^T \\).\n",
    "\n",
    "### Applications in Data Science\n",
    "\n",
    "Eigenvalues and eigenvectors play a critical role in many machine learning and data science techniques:\n",
    "1. **Principal Component Analysis (PCA)**: PCA uses eigenvalues and eigenvectors to reduce the dimensionality of data by selecting the top \\( k \\) eigenvectors corresponding to the largest eigenvalues.\n",
    "2. **Spectral Clustering**: Uses the eigenvectors of a similarity matrix to perform clustering.\n",
    "3. **Latent Semantic Analysis (LSA)**: SVD and eigen-decomposition are used for dimensionality reduction in text mining and information retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Understanding eigenvalues and eigenvectors is fundamental in many areas of machine learning, data science, and linear algebra. The ability to compute and interpret them enables powerful tools such as PCA, SVD, and spectral methods for clustering, dimensionality reduction, and matrix factorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
