{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Angle Regression (LARS)\n",
    "\n",
    "Least Angle Regression (LARS) is an algorithm used for model selection and regularization in linear regression, particularly when the number of predictors is large relative to the number of observations. It is closely related to **Lasso Regression** and **Forward Stepwise Regression**, and can be thought of as a faster, more efficient approach to solving problems where regularization (such as L1 regularization) is used to prevent overfitting.\n",
    "\n",
    "## Key Concepts in LARS\n",
    "\n",
    "LARS is primarily used when we have high-dimensional data where the number of features (predictors) \\( p \\) is much larger than the number of observations \\( n \\), or when we want to select a sparse set of important predictors from many possible ones. LARS is particularly useful when the solution involves a sparse set of non-zero coefficients, similar to Lasso.\n",
    "\n",
    "### LARS and Lasso\n",
    "\n",
    "In the context of Lasso, the goal is to minimize the following objective function:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\beta)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the observed response for the \\( i \\)-th observation,\n",
    "- \\( \\mathbf{x}_i \\) is the feature vector for the \\( i \\)-th observation,\n",
    "- \\( \\beta_j \\) is the coefficient for the \\( j \\)-th predictor,\n",
    "- \\( \\lambda \\) is the regularization parameter controlling the sparsity of the model (larger \\( \\lambda \\) results in more regularization, forcing more coefficients to zero).\n",
    "\n",
    "LARS is an efficient algorithm for solving this optimization problem. It can be seen as a method for performing **forward stepwise regression** that proceeds in a way similar to **Lasso**, where it adds predictors into the model one by one, but with adjustments made during each step to encourage sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "## LARS Algorithm\n",
    "\n",
    "The LARS algorithm operates as follows:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with all coefficients \\( \\beta_j = 0 \\) for all \\( j \\). \n",
    "   - Calculate the residuals: \\( r = y - X\\beta \\), where \\( X \\) is the design matrix of the input features, and \\( y \\) is the target vector.\n",
    "\n",
    "2. **Iterative Process**:\n",
    "   - Identify the predictor \\( j \\) that is most correlated with the residuals \\( r \\), i.e., the one that maximizes \\( |X_j^T r| \\).\n",
    "   - Add this predictor to the active set of predictors (denoted as \\( A \\)).\n",
    "   - Move the coefficients of the active predictors towards their least-squares estimates. The update for the coefficients is done in the direction of the predictor with the largest correlation with the residuals.\n",
    "   - The step size \\( \\gamma \\) is chosen such that the next predictorâ€™s coefficient will hit zero at the same time as the current one. This ensures sparsity and efficient selection of predictors.\n",
    "\n",
    "3. **Stopping Criteria**:\n",
    "   - The process continues until either all predictors have been added or until the desired sparsity (or number of non-zero coefficients) is achieved.\n",
    "   - The regularization parameter \\( \\lambda \\) can be varied to control the amount of shrinkage applied to the coefficients.\n",
    "\n",
    "4. **Path of Solutions**:\n",
    "   - LARS can be used to generate a **path of solutions** by adjusting \\( \\lambda \\). At each value of \\( \\lambda \\), a different subset of predictors is selected, leading to a sequence of models with increasing sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "The LARS algorithm solves the following objective function in a stepwise manner:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\|y - X\\beta\\|_2^2 \\) is the residual sum of squares (RSS),\n",
    "- \\( \\|\\beta\\|_1 = \\sum_{j=1}^{p} |\\beta_j| \\) is the L1 norm of the coefficients, which encourages sparsity.\n",
    "\n",
    "At each iteration:\n",
    "1. **Compute correlations**: The correlation between each feature and the current residuals is calculated.\n",
    "2. **Choose the most correlated feature**: The feature with the highest absolute correlation is added to the active set of features.\n",
    "3. **Update coefficients**: The coefficients of the active features are updated, and the residuals are recalculated.\n",
    "4. **Repeat** until the stopping criteria are met.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison to Other Methods\n",
    "\n",
    "- **Lasso Regression**: LARS is closely related to Lasso regression but is often computationally more efficient, especially when the number of features is large. LARS provides a solution path for all possible values of \\( \\lambda \\), which can be computed more quickly than directly solving the Lasso problem via coordinate descent.\n",
    "  \n",
    "- **Forward Stepwise Regression**: LARS is similar to forward stepwise regression, but it moves the coefficients in a way that is more akin to the path of least angle, ensuring that predictors enter the model in a more optimized manner.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of LARS\n",
    "\n",
    "1. **High-Dimensional Data**: LARS is particularly useful for problems with a large number of predictors (features), where a sparse model is desirable. Examples include genomic data analysis, text classification, and image recognition, where the number of features can far exceed the number of observations.\n",
    "  \n",
    "2. **Variable Selection**: LARS helps identify the most relevant features for the model, providing an automatic method for feature selection in high-dimensional settings. It is an excellent alternative to stepwise regression techniques in selecting sparse sets of predictors.\n",
    "\n",
    "3. **Sparse Solutions**: LARS is ideal when a sparse model (with many coefficients set to zero) is desired. This is particularly useful in cases where we expect only a small subset of features to contribute meaningfully to the outcome (e.g., in signal processing or sparse representation).\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of LARS\n",
    "\n",
    "- **Computational Efficiency**: LARS is faster than solving the Lasso problem using coordinate descent, especially in high-dimensional data. It computes the entire path of solutions in a single run.\n",
    "- **Sparsity**: LARS naturally produces sparse models, which is beneficial when dealing with a large number of predictors, ensuring that only the most important features are selected.\n",
    "- **Regularization**: LARS provides a regularization path, which allows for flexibility in choosing the optimal level of regularization for the problem at hand.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Method        | Type of Regression   | Regularization    | Key Features                              |\n",
    "|---------------|----------------------|-------------------|-------------------------------------------|\n",
    "| **LARS**      | Linear Regression    | L1 (Lasso)        | Efficient path algorithm for Lasso; Sparse solutions, good for high-dimensional data |\n",
    "| **Lasso**     | Linear Regression    | L1 (Lasso)        | Regularization with sparse coefficient estimates, but slower than LARS |\n",
    "| **Stepwise**  | Linear Regression    | None (Variable Selection) | Selects features one at a time, often computationally expensive |\n",
    "\n",
    "In conclusion, Least Angle Regression (LARS) is a powerful and computationally efficient method for solving linear regression problems with high-dimensional data, especially when sparsity is required. It is particularly valuable in situations where feature selection and regularization are critical, making it an essential tool in modern statistical and machine learning workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
