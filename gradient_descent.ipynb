{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize a function by adjusting its parameters in the direction of the steepest decrease of the function. It is widely used in machine learning, especially for training models like linear regression, logistic regression, and neural networks. The core idea is to update the model parameters to reduce the error or cost function over time.\n",
    "\n",
    "## General Formulation of Gradient Descent\n",
    "\n",
    "Given a function \\( f(\\theta) \\), where \\( \\theta \\) represents the parameters of the model (e.g., weights in a neural network), the goal of gradient descent is to find the minimum of this function. The update rule for gradient descent is:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\theta_t \\) is the value of the parameters at step \\( t \\),\n",
    "- \\( \\alpha \\) is the learning rate, a small positive scalar that controls the step size,\n",
    "- \\( \\nabla f(\\theta_t) \\) is the gradient of the function \\( f(\\theta) \\) with respect to the parameters \\( \\theta \\) at step \\( t \\),\n",
    "- \\( \\theta_{t+1} \\) is the updated value of the parameters after applying the gradient update.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Gradient**: The gradient \\( \\nabla f(\\theta) \\) is the vector of partial derivatives of \\( f(\\theta) \\) with respect to each parameter. It points in the direction of the steepest ascent, and its negative points in the direction of the steepest descent.\n",
    "  \n",
    "- **Learning Rate (\\( \\alpha \\))**: The learning rate determines how large a step is taken along the negative gradient. If \\( \\alpha \\) is too small, the algorithm may converge slowly; if \\( \\alpha \\) is too large, the algorithm might overshoot the minimum and fail to converge.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent for Linear Regression\n",
    "\n",
    "In linear regression, the goal is to minimize the **cost function**, often represented by the **Mean Squared Error (MSE)**. The cost function for linear regression is defined as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( m \\) is the number of training examples,\n",
    "- \\( h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1 x^{(i)} \\) is the hypothesis (the prediction of the model),\n",
    "- \\( y^{(i)} \\) is the actual target value for the \\( i \\)-th training example,\n",
    "- \\( \\theta \\) represents the model parameters (weights).\n",
    "\n",
    "The gradient of the cost function \\( J(\\theta) \\) with respect to \\( \\theta \\) is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "### Gradient Descent Update Rule for Linear Regression:\n",
    "Using the gradient descent formula, the parameters are updated as:\n",
    "\n",
    "$$\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "This process is repeated iteratively, adjusting the values of \\( \\theta_0 \\) and \\( \\theta_1 \\) until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Gradient Descent\n",
    "\n",
    "There are several variations of gradient descent, each differing in how the data is processed during the update step:\n",
    "\n",
    "### 1. **Batch Gradient Descent**\n",
    "\n",
    "In Batch Gradient Descent, the gradients are computed using the entire training dataset. The update rule is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "Where \\( \\nabla J(\\theta) \\) is the gradient calculated using the entire dataset. This approach can be very slow for large datasets, as it requires computing gradients for all examples in each iteration.\n",
    "\n",
    "### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "Stochastic Gradient Descent (SGD) computes the gradient and updates the parameters using only a single randomly selected training example in each iteration. The update rule is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla J(\\theta^{(i)})\n",
    "$$\n",
    "\n",
    "Where \\( \\nabla J(\\theta^{(i)}) \\) is the gradient computed using only the \\( i \\)-th training example. Since it uses only one data point, it is much faster per iteration but introduces noise in the updates, which can help escape local minima.\n",
    "\n",
    "### 3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "Mini-Batch Gradient Descent is a compromise between Batch Gradient Descent and Stochastic Gradient Descent. In Mini-Batch Gradient Descent, the data is divided into small batches, and the gradient is computed for each mini-batch. The update rule is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla J(\\theta_{\\text{mini-batch}})\n",
    "$$\n",
    "\n",
    "Where the gradient is averaged over a small subset of data, typically in the range of 32 to 512 examples. This method provides a balance between computational efficiency and the noisy updates of SGD.\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence of Gradient Descent\n",
    "\n",
    "The convergence of Gradient Descent depends on several factors:\n",
    "1. **Learning Rate**: If the learning rate is too large, gradient descent may diverge, while a very small learning rate may lead to slow convergence.\n",
    "2. **Convexity of the Function**: If the function \\( f(\\theta) \\) is convex, gradient descent will converge to the global minimum. For non-convex functions, gradient descent may converge to a local minimum.\n",
    "3. **Momentum and Adaptive Learning Rates**: Techniques like momentum, Adagrad, Adam, and RMSprop can be used to improve the convergence speed and stability by adjusting the learning rate dynamically or using past gradients to smooth out the updates.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Gradient Descent for Logistic Regression\n",
    "\n",
    "For logistic regression, the cost function (cross-entropy) is given by:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "Where \\( h_\\theta(x^{(i)}) = \\frac{1}{1 + e^{-\\theta^T x^{(i)}}} \\) is the logistic function (sigmoid).\n",
    "\n",
    "The gradient of \\( J(\\theta) \\) with respect to \\( \\theta \\) is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "The gradient descent update rule for logistic regression is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "This update is repeated iteratively until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Gradient Descent is a versatile optimization technique that plays a crucial role in training machine learning models. By iteratively adjusting the model's parameters in the direction of the negative gradient, gradient descent minimizes the cost function, enabling the model to learn from data. Different variations of gradient descent (batch, stochastic, mini-batch) offer trade-offs between computational efficiency and convergence speed, making the algorithm suitable for a wide range of machine learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
